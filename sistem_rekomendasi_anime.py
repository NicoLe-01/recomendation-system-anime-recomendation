# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Anime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oK3HZ4kRW7gHrjzkpFz2Jg-Y2laOd0jJ

Dataset : https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database
"""

!unzip '/content/drive/MyDrive/Dataset_Kaggle/Anime Recommendations Database.zip' -d '/content/dataset'

"""Import library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from collections import defaultdict

"""Baca dataset"""

animes = pd.read_csv('/content/dataset/anime.csv')
ratings = pd.read_csv('/content/dataset/rating.csv')

animes.head()

animes.info()

"""Dapat dilihat bahwa terdapat 12294 judul anime pada dataset"""

print('banyak data : ', len(animes.anime_id.unique()))
print('jenis genre : ', animes.genre.unique())

print(animes.type.unique())

"""Cek nilai null"""

animes.isnull().sum()

"""Hapus nilai null"""

animes.dropna(inplace=True)

animes.isnull().sum()

"""Hitung banyak genre lalu dimasukkan ke dalam dictionary"""

all_genres = defaultdict(int)

for genres in animes['genre']:
    for genre in genres.split(','):
        all_genres[genre.strip()] += 1

all_genres

"""Pesebaran genre anime"""

from wordcloud import WordCloud

from collections import defaultdict

all_genres = defaultdict(int)

for genres in animes['genre']:
    for genre in genres.split(','):
        all_genres[genre.strip()] += 1

def wordCloud(words):
    wordCloud = WordCloud(width=1000, height=800, background_color='white').generate_from_frequencies(words)

    plt.imshow(wordCloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

wordCloud(all_genres)

"""# **Content Base Filltering**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tf = TfidfVectorizer() # inisiasi 
tf.fit(animes['genre'])
tf.get_feature_names()

tfidf_matrix = tf.fit_transform(animes['genre'])

tfidf_matrix.shape

tfidf_matrix.todense()

"""Cek vektor"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names(),
    index=animes.name
).sample(22, axis=1).sample(10, axis=0)

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=animes['name'], columns=animes['name'])

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

cosine_sim_df

def anime_recommendation(nama_anime, similarity_data=cosine_sim_df, items=animes[['name', 'genre']], k=5):
  index = similarity_data.loc[:, nama_anime].to_numpy().argpartition(
      range(-1, -k, -1)
  )

  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  closest = closest.drop(nama_anime, errors='ignore')
  df = pd.DataFrame(closest).merge(items).head(k)
  return df

"""Hasil rekomendasi"""

animes[animes['name'] == "Sword Art Online"]

anime_recommendation('Sword Art Online', k=5)

"""Evaluasi model menggunakan Jaccard Similarity"""

def jaccard_set(list_genre, list_genre_prediksi):
    intersection = len(list(set(list_genre).intersection(list_genre_prediksi)))
    union = (len(list_genre) + len(list_genre_prediksi)) - intersection
    return float(intersection) / union

genre_anime  = animes[animes.name.eq('Sword Art Online')]['genre'].values.tolist()

def to_list(list):
  for i in list:
    return i.split(',')

genre_anime = to_list(genre_anime)

all_genre_prediksi = anime_recommendation('Sword Art Online')['genre'].values

def jaccard_value_total(genre_anime, genre_prediksi):
  total_jaccard_value = 0
  result = 0
  for i in range(len(genre_prediksi)):
    genre = genre_prediksi[i].split(',')
    jaccard_value = jaccard_set(genre_anime, genre)
    total_jaccard_value += jaccard_value
    result = total_jaccard_value / len(genre_prediksi)
  return result

jaccard_value_total(genre_anime, all_genre_prediksi)

"""Dapat dilihat, jaccard score menunjukkan 0.8, yang artinya rekomendasi yang diberikan berdasarkan genre cukup baik.

# **Collaborative Filltering**

Cek data rating
"""

ratings.head()

"""Hilangkan nilai -1, karena nilai tersebut nilai null/orang yang belum/tidak memberikan rating.
Dengan menggunakan operator bitwise
"""

mask = (ratings['rating'] == -1)

ratings = ratings.loc[~mask]

ratings[ratings['rating'] == -1].sum()

"""ambil 1000 data rating"""

ratings = ratings[ratings['user_id'] < 1000]

"""encoding rating user_id"""

from sklearn.preprocessing import LabelEncoder
encoder_rating = LabelEncoder()

ratings['user_id'] = encoder_rating.fit_transform(ratings['user_id'])

"""encoding anime_id"""

encoder_anime = LabelEncoder()

ratings['anime_id'] = encoder_anime.fit_transform(ratings['anime_id'])

"""ambil nilai unique"""

userid_unique = ratings['user_id'].nunique()
anime_unique = ratings['anime_id'].nunique()

print("user unique : ", userid_unique)
print("anime unique : ", anime_unique)

"""import library"""

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Embedding, Reshape, Flatten, concatenate, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split

X = ratings.drop(['rating'], axis=1)

y = ratings['rating'].astype(float)

"""Bagi data"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=22)

"""buat callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoc_end(self, epoch, logs={}):
    if(logs.get('mean_squared_error') <= 0.6):
      print("\nMean squared error telah mencapai 0.6. Stop training!")
      self.model.stop_training = True
my_callbacks = myCallback()

"""Model"""

def RecommenderAnime(n_users, n_movies, n_dim):
    
    # User
    user = Input(shape=(1,))
    U = Embedding(n_users, n_dim)(user)
    U = Dropout(0.2)(U)
    U = Flatten()(U)
    
    # Anime
    movie = Input(shape=(1,))
    M = Embedding(n_movies, n_dim)(movie)
    M = Dropout(0.2)(M)
    M = Flatten()(M)
    
    # Gabungkan disini
    merged_vector = concatenate([U, M])
    dense_1 = Dense(128, activation='relu')(merged_vector)
    dropout = Dropout(0.3)(dense_1)
    final = Dense(1)(dropout)
    
    model = Model(inputs=[user, movie], outputs=final)
    
    model.compile(optimizer=Adam(0.001),
                  loss='mean_squared_error',
                  metrics=['mse'])
    
    return model

model = RecommenderAnime(userid_unique, anime_unique, 100)

model.summary()

"""Traning model"""

history = model.fit(x=[X_train['user_id'], X_train['anime_id']],
                    y=y_train,
                    batch_size=8,
                    epochs=30,
                    verbose=1,
                    validation_data=([X_val['user_id'], X_val['anime_id']], y_val),
                    callbacks=[my_callbacks]
                    )

"""plot loss model"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

plt.plot(history.history['mse'])
plt.plot(history.history['val_mse'])
plt.title('Nilai MSE model')
plt.ylabel('mse')
plt.xlabel('epochs')
plt.legend(['mse', 'val_mse'], loc='upper right')
plt.show()

"""simpan model"""

model.save('my_model.h5')

"""buat fungsi prediksi"""

def buat_prediksi(user_id, anime_id, model):
    return model.predict([np.array([user_id]), np.array([anime_id])])[0][0].round(2)



def prediksi_teratas(user_id, model, k):
  """
  Parameter user_id : input user ke n dari data set
  Parameter model : input model yang telah di traning
  Parameter k : input berapa banyak prediksi yang akan tampilkan
  """


  user_id = int(user_id) - 1    # ambil user id
  user_ratings = ratings[ratings['user_id'] == user_id] # lihat anime apa saja yg telah di review oleh pengguna
  anime_id_user_ratings = user_ratings.anime_id.values.tolist() # buatlist anime yang telah ditonton oleh pengguna
  anime_viewed = animes.loc[animes['anime_id'].isin(anime_id_user_ratings)].sort_values(by='anime_id')  # cari anime berdasarkan anime_id
  
  genre_viewed = defaultdict(int)  # buat dictionary

  for genres in anime_viewed['genre']:  # hitung genre yang muncul
    for genre in genres.split(','):
        genre_viewed[genre.strip()] += 1

  genre_viewed_by_user = list(sorted(genre_viewed, key=genre_viewed.get, reverse=True)) # urutkan genre yang muncul dari yang paling banyak
  top7_genre_by_user = genre_viewed_by_user[:7]  # ambil 7 genre tertinggi

  print("7 genre yang banyak ditonton oleh pengguna : \n")
  print(top7_genre_by_user, sep=", ")
  print("=======================" * 5)
  
  rekomendasi = ratings[~ratings['anime_id'].isin(user_ratings['anime_id'])][['anime_id']].drop_duplicates() # hilangkan anime yg telah di review dan masukkan ke dataframe rekomendasi
  rekomendasi['rating_predict'] = rekomendasi.apply(lambda x: buat_prediksi(user_id, x['anime_id'], model), axis=1) # prediksi semua baris data anime. yg hasilnya dimasukkan ke dataframe rekomendasi
  rekomendasi_fix = rekomendasi.sort_values(by='rating_predict', ascending=False).merge(animes[['anime_id', 'name', 'type', 'members', 'genre']],
                                                                                       on='anime_id').head(k) # urutkan dari rating 5 tertinggi 
  return rekomendasi_fix.sort_values('rating_predict', ascending=False)[['name', 'type', 'rating_predict', 'genre']]

"""hasil prediksi"""

prediksi_teratas(200, model, 5)

prediksi_teratas(34, model, 5)

prediksi_teratas(45, model, 5)